#include "NeuralNetwork.h"

using namespace std;

Scalar activationFunction(Scalar x)
{
    // –Ω–µ–ª—ñ–Ω—ñ–π–Ω–∞ —Ñ—É–Ω–∫—Ü—ñ—è –¥–ª—è –∞–∫—Ç–∏–≤–∞—Ü—ñ—ó
    return tanhf(x);
}

Scalar activationFunctionDerivative(Scalar x)
{
    // –ø–æ—Ö—ñ–¥–Ω–∞
    return 1 + tanhf(x) * tanhf(x);
}

NeuralNetwork::NeuralNetwork(vector<uint> topology, Scalar learningRate)
{
    this->topology = topology;
    this->learningRate = learningRate;
    for (int i = 0; i < topology.size(); i++)
    {
        if (i == topology.size() - 1)
        {
            neuronLayers.push_back(new RowVector(topology[i]));
        }
        else
        {
            neuronLayers.push_back(new RowVector(topology[i] + 1));
        }
        cacheLayers.push_back(new RowVector (neuronLayers.size()));
        deltas.push_back((new RowVector(neuronLayers.size())));
        if (i != topology.size() - 1)
        {
            neuronLayers.back()->coeffRef(topology[i]) = 1.0;
            cacheLayers.back()->coeffRef(topology[i]) = 1.0;
        }
        if (i > 0)
        {
            if (i != topology.size() - 1)
            {
                weights.push_back(new Matrix(topology[i - 1] + 1, topology[i] + 1));
                weights.back()->setRandom();
                weights.back()->col(topology[i]).setZero();
                weights.back()->coeffRef(topology[i - 1], topology[i]) = 1.0;
            }
            else
            {
                weights.push_back(new Matrix(topology[i - 1] + 1, topology[i]));
                weights.back()->setRandom();
            }
        }
    }
}

void NeuralNetwork::propagateForward(RowVector& input)
{
    // —â–æ–± –Ω–µ–π—Ä–æ–Ω–∏ –∑–º—ñ—â–µ–Ω–Ω—è –∑–∞–ª–∏—à–∏–ª–∏—Å—è –±–µ–∑ –∑–º—ñ–Ω —Ä–æ–±–∏–º–æ
    // –≤—Å—ñ –µ–ª–µ–º–µ–Ω—Ç–∏ –æ—Å—Ç–∞–Ω–Ω—å–æ–≥–æ —Å—Ç–æ–≤–ø—á–∏–∫–∞ 0, –æ–∫—Ä—ñ–º –æ—Å—Ç–∞–Ω–Ω—å–æ–≥–æ
    neuronLayers.front()->block(0, 0, 1, neuronLayers.front()->size() - 1);
    for (int i = 1; i < topology.size(); i++)
    {
        // –ø–æ—à–∏—Ä—é—î–º–æ –¥–∞–Ω—ñ –∑–∞—Å—Ç–æ—Å–æ–≤—É—é—á–∏ —Ñ—É–Ω–∫—Ü—ñ—é –∞–∫—Ç–∏–≤–∞—Ü—ñ—ó –¥–ª—è –∫–æ–∂–Ω–æ–≥–æ –µ–ª–µ–º–µ–Ω—Ç–∞
        (*neuronLayers[i]) = (*neuronLayers[i - 1]) * (*weights[i - 1]);
        neuronLayers[i]->block
                (0, 0, 1, topology[i]).unaryExpr(cref(activationFunction));
    }
}

void NeuralNetwork::propagateBackward(RowVector &output)
{
    calcErrors(output);
    updateWeights();
}

void NeuralNetwork::calcErrors(RowVector& output)
{
    // –ø–æ–º–∏–ª–∫–∏ –∑—Ä–æ–±–ª–µ–Ω—ñ –Ω–µ–π—Ä–æ–Ω–∞–º–∏ –æ—Å—Ç–∞–Ω–Ω—å–æ–≥–æ —à–∞—Ä—É
    (*deltas.back()) = output - (*neuronLayers.back());

    // –ø–æ–º–∏–ª–∫–∏ –∑—Ä–æ–±–ª–µ–Ω—ñ –Ω–µ–π—Ä–æ–Ω–∞–º–∏ –ø—Ä–∏—Ö–æ–≤–∞–Ω–∏—Ö —à–∞—Ä—ñ–≤
    // —Ä–∞—Ö—É—î–º–æ –≤—ñ–¥ –æ—Å—Ç–∞–Ω–Ω—å–æ–≥–æ –¥–æ –ø–µ—Ä—à–æ–≥–æ
    for (uint i = topology.size() - 2; i > 0; i--)
    {
        (*deltas[i]) = (*deltas[i - 1]) * (weights[i]->transpose());
    }
}

void NeuralNetwork::updateWeights()
{
    // topology.size()-1 = weights.size()
    for (int i = 0; i < topology.size() - 1; i++)
    {
        // –¥–ª—è –∫–æ–∂–Ω–æ–≥–æ —à–∞—Ä—É –Ω–µ–π—Ä–æ–º–µ—Ä–µ–∂—ñ
        // —è–∫—â–æ —Ü–µ output, —Ç–æ –Ω–µ–π—Ä–æ–Ω–∞ –∑–º—ñ—â–µ–Ω–Ω—è –Ω–µ–º–∞—î —ñ –≤–∫–∞–∑–∞–Ω–∞ –∫-—Å—Ç—å –Ω–µ–π—Ä–æ–Ω—ñ–≤ = –∫-—Å—Ç—å —Å—Ç–æ–≤–ø—á–∏–∫—ñ–≤
        // —è–∫—â–æ –Ω—ñ, —Ç–æ –≤–∫–∞–∑–∞–Ω–∞ –∫-—Å—Ç—å –Ω–µ–π—Ä–æ–Ω—ñ–≤ = –∫-—Å—Ç—å —Å—Ç–æ–≤–ø—á–∏–∫—ñ–≤ - 1

        /*
        c-–π —Å—Ç–æ–≤–ø–µ—Ü—å —É –º–∞—Ç—Ä–∏—Ü—ñ –≤–∞–≥–∏ –ø—Ä–µ–¥—Å—Ç–∞–≤–ª—è—î –∑'—î–¥–Ω–∞–Ω–Ω—è c-–≥–æ –Ω–µ–π—Ä–æ–Ω–∞ –≤ CURRENT_LAYER –¥–æ –≤—Å—ñ—Ö –Ω–µ–π—Ä–æ–Ω—ñ–≤ PREV_LAYER.
        r-–π –µ–ª–µ–º–µ–Ω—Ç c-–≥–æ —Å—Ç–æ–≤–ø—Ü—è –≤ –º–∞—Ç—Ä–∏—Ü—ñ –≤–∞–≥–∏ –ø—Ä–µ–¥—Å—Ç–∞–≤–ª—è—î –∑'—î–¥–Ω–∞–Ω–Ω—è c-–≥–æ –Ω–µ–π—Ä–æ–Ω–∞ –≤ CURRENT_LAYER –¥–æ r-–≥–æ –Ω–µ–π—Ä–æ–Ω–∞ –≤ PREV_LAYER.
        r-–π —Ä—è–¥–æ–∫ —É –º–∞—Ç—Ä–∏—Ü—ñ –≤–∞–≥–∏ –ø—Ä–µ–¥—Å—Ç–∞–≤–ª—è—î –∑'—î–¥–Ω–∞–Ω–Ω—è –≤—Å—ñ—Ö –Ω–µ–π—Ä–æ–Ω—ñ–≤ PREV_LAYER –∑ r-–∏–º –Ω–µ–π—Ä–æ–Ω–æ–º —É CURRENT_LAYER.
        c-–∏–π –µ–ª–µ–º–µ–Ω—Ç r-–≥–æ —Ä—è–¥–∫–∞ –≤ –º–∞—Ç—Ä–∏—Ü—ñ –≤–∞–≥–∏ –ø—Ä–µ–¥—Å—Ç–∞–≤–ª—è—î –∑'—î–¥–Ω–∞–Ω–Ω—è c-–≥–æ –Ω–µ–π—Ä–æ–Ω–∞ –≤ PREV_LAYER –¥–æ r-–≥–æ –Ω–µ–π—Ä–æ–Ω–∞ –≤ CURRENT_LAYER.
         */
        if ( i != topology.size() - 2)
        {
            for (int c = 0; c < weights[i]->cols() - 1; c++)
            {
                for (int r = 0; r < weights[i]->rows(); r++)
                {
                    // –∑–∞ —Ñ–æ—Ä–º—É–ª–æ—é
                    weights[i]->coeffRef(r, c) += learningRate * deltas[i + 1]->coeffRef(c)
                                                  * activationFunctionDerivative(cacheLayers[i + 1]->coeffRef(c))
                                                  * neuronLayers[i]->coeffRef(r);
                }
            }
        }
        else
        {
            for (int c = 0; c < weights[i]->cols(); c++)
            {
                for (int r = 0; r < weights[i]->rows(); r++)
                {
                    weights[i]->coeffRef(r, c) += learningRate * deltas[i + 1]->coeffRef(c)
                                                  * activationFunctionDerivative(cacheLayers[i + 1]->coeffRef(c))
                                                  * neuronLayers[i]->coeffRef(r);
                }
            }
        }
    }
}

void NeuralNetwork::train(vector<RowVector*> input_data, vector<RowVector*> output_data)
{
    for (int i = 0; i < input_data.size(); i++)
    {
        cout << "–í—Ö—ñ–¥–Ω—ñ –¥–∞–Ω—ñ –Ω–µ–π—Ä–æ–Ω–Ω–æ—ó –º–µ—Ä–µ–∂—ñ : " << *input_data[i] << endl;
        propagateForward(*input_data[i]);
        cout << "–û—á—ñ–∫—É—î–º–∏–π —Ä–µ–∑—É–ª—å—Ç–∞—Ç : " << *output_data[i] << endl;
        cout << "–û—Ç—Ä–∏–º–∞–Ω–∏–π —Ä–µ–∑—É–ª—å—Ç–∞—Ç : " << *neuronLayers.back() << endl;
        propagateBackward(*output_data[i]);
        // ùëÄùëÜùê∏ = 1/ùëõ * ‚àëùëõùëñ(ùë¶ùëñ‚àíùë¶ùëñ')^2
        cout << "MSE (—Å–µ—Ä–µ–¥–Ω—è –∫–≤–∞–¥—Ä–∞—Ç–∏—á–Ω–∞ –ø–æ–º–∏–ª–∫–∞): " << endl;
        cout << sqrt((*deltas.back()).dot((*deltas.back())) / (float)deltas.back()->size());
        cout << endl;
    }
}
